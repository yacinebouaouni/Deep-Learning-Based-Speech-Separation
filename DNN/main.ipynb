{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "shgQ2hvdBkLT"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io.wavfile import read, write\n",
        "from scipy import signal\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from numpy.linalg import inv\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as I\n",
        "from torch.nn import Sigmoid\n",
        "\n",
        "\n",
        "from torch import transpose\n",
        "from sklearn.neural_network import BernoulliRBM\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "import numpy as np\n",
        "from scipy.signal import butter, lfilter, freqz\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF86xdXcKGn6",
        "outputId": "f2d7654b-4171-4b83-9308-5ee9e47b08fe"
      },
      "source": [
        "from helpers import *\n",
        "#from nmf_utils import *\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbdrXIYx9tNS"
      },
      "source": [
        "!ffmpeg -i  \"/content/drive/MyDrive/vocal_10.mp3\" \"vocal.wav\";\n",
        "!ffmpeg -i  \"/content/drive/MyDrive/piano_10.mp3\" \"music.wav\";"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imGnRSmvgI84",
        "outputId": "c747d47f-83f7-4628-caee-0a99d8776953"
      },
      "source": [
        "path_music  = \"music.wav\"\n",
        "path_speech = \"vocal.wav\"\n",
        "sec = 1088\n",
        "music,speech_t,samplerate = load_data(path_music,path_speech,sec)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the music 47980800\n",
            "Length : 1088.00s\n",
            "Sample rate : 44100\n",
            "Shape of the speech 47980800\n",
            "Length : 1088.00s\n",
            "Sample rate : 44100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUWMVpxMYvYY"
      },
      "source": [
        "WINDOW = 'hamming'\n",
        "WINDOW_SIZE=480\n",
        "OVERLAP = 0.6 * WINDOW_SIZE\n",
        "NFFT=512"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2EUU-uGSp0H"
      },
      "source": [
        "from scipy import signal"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dySIXiXiShvO"
      },
      "source": [
        "rate = 44100/16000\n",
        "speech_t = signal.resample(speech_t,int(speech_t.shape[0]/rate))\n",
        "music=signal.resample(music,int(music.shape[0]/rate))\n",
        "samplerate_m=int(samplerate/rate)\n",
        "samplerate_s=samplerate_m\n",
        "samplerate_t=samplerate_m"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoLZWdzPpTyK",
        "outputId": "2a44feaa-1813-4ad8-b668-5963e55b877d"
      },
      "source": [
        "samplerate_t"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3wqL7FBcatQ"
      },
      "source": [
        "# NMF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6d5ndYPG-3b"
      },
      "source": [
        "# Train / Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0B-q0YTIZAt"
      },
      "source": [
        "speech = butter_lowpass_filter(speech_t,4000,16000)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mOVJq1h4Eqq"
      },
      "source": [
        "train_speech   = np.hstack((speech[samplerate_t*60:samplerate_t*440] , speech[samplerate_t*723:samplerate_t*930]))\n",
        "train_music    = np.hstack((music[samplerate_t*60:samplerate_t*440] ,  music[samplerate_t*723:samplerate_t*930]))\n",
        "\n",
        "test_speech ,test_music =  speech[samplerate_t*580:samplerate_t*700]  , music[samplerate_t*580:samplerate_t*700]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5_6L38dY91q",
        "outputId": "e48bfd99-fa4b-46e1-c0f9-2e8d6c931e35"
      },
      "source": [
        "test_speech.shape, test_music.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1920000,), (1920000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhGfvMxh9s1w"
      },
      "source": [
        "## Test different configurations 5,8,10,32 components"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6852M_ay2UN"
      },
      "source": [
        "B_all_list= []\n",
        "for c in [10]:\n",
        "  B_mixed = training(train_music,\n",
        "               train_speech,\n",
        "               s_component = c,\n",
        "               m_component = c,\n",
        "               iterations = 100,\n",
        "               s_alpha = 0,\n",
        "               m_alpha = 0,\n",
        "               p = 2,\n",
        "               init=\"random\")\n",
        "  B_mixed_new = MinMaxScaler().fit_transform(B_mixed)\n",
        "  B_all_list.append(B_mixed_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6SNwvD4ND-z"
      },
      "source": [
        "# Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "A8GSFmRYYRP1",
        "outputId": "f954999b-7153-4b78-d1dd-dd40ba3fbe30"
      },
      "source": [
        "B_mixed = np.load(\"b_train.npy\")\n",
        "my_dict_2= {}\n",
        "info_dict_2 = {} \n",
        "for s in [-5]:\n",
        "  smr,sdr_speech,sdr_music,spectrum_s1,spectrum_s2,Y_mag = validation(B_mixed,test_speech, ## n component = 8 +8\n",
        "                  test_music,\n",
        "                  smr=s,\n",
        "                  iterations=100,\n",
        "                  m_component=8,\n",
        "                  s_component=8,\n",
        "                  alpha=0,\n",
        "                  init=\"random\"\n",
        ")\n",
        "  \n",
        "  my_dict_2[\"SMR = {}\".format(smr)] = info_dict_2 = [\"SDR Speech = {:.2f} SDR Music = {:.2f}\".format(sdr_speech,sdr_music)]\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SMR = -5.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1b81d3a2a9e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                   \u001b[0ms_component\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                   \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                   \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"random\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/helpers.py\u001b[0m in \u001b[0;36mvalidation\u001b[0;34m(B_mixed, speech, music, s_component, m_component, alpha, init, smr, iterations, samplerate)\u001b[0m\n\u001b[1;32m    304\u001b[0m               \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m               \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m               \u001b[0minit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m               )\n\u001b[1;32m    308\u001b[0m   \u001b[0;31m#B_mixed,G_mixed,error_speech = speech_nmf(mixed_abs,s_component,iterations,0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/helpers.py\u001b[0m in \u001b[0;36mmixed_nmf\u001b[0;34m(B_mixed, Y_test, iterations, a, initt)\u001b[0m\n\u001b[1;32m    270\u001b[0m               random_state=0)\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB_mixed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_nmf.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1310\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \"\"\"\n\u001b[0;32m-> 1312\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_nmf.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, W, H)\u001b[0m\n\u001b[1;32m   1285\u001b[0m             \u001b[0ml1_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'both'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m             \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1287\u001b[0;31m             shuffle=self.shuffle)\n\u001b[0m\u001b[1;32m   1288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m         self.reconstruction_err_ = _beta_divergence(X, W, H, self.beta_loss,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_nmf.py\u001b[0m in \u001b[0;36mnon_negative_factorization\u001b[0;34m(X, W, H, n_components, init, update_H, solver, beta_loss, tol, max_iter, alpha, l1_ratio, regularization, random_state, verbose, shuffle)\u001b[0m\n\u001b[1;32m   1067\u001b[0m                                                   \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_reg_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_reg_H\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m                                                   \u001b[0ml2_reg_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_reg_H\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_H\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m                                                   verbose)\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_nmf.py\u001b[0m in \u001b[0;36m_fit_multiplicative_update\u001b[0;34m(X, W, H, beta_loss, max_iter, tol, l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H, update_H, verbose)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0;31m# test convergence criterion every 10 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtol\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_beta_divergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquare_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/decomposition/_nmf.py\u001b[0m in \u001b[0;36m_beta_divergence\u001b[0;34m(X, W, H, beta, square_root)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_data\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mWH_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# beta-divergence, beta not in (0, 1, 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MfTxppWL1QS"
      },
      "source": [
        "# RBM Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_YE9LDMMXTc"
      },
      "source": [
        "from helpers_rbm import *\n",
        "def train_layer( visible_size,hidden_size,      # this function allows us to train every layer separately, considering new dataloader that will feed into the new visible layer\n",
        "                 s1,s2,\n",
        "                 learning_rate,\n",
        "                 epochs=2,\n",
        "                 batch_size=256\n",
        "                ):\n",
        "  model = RBM(visible_size, hidden_size)        # Declare the model\n",
        "  dataset=AudioDataset(Y_mag_scaled, s1, s2)   # Dataloader\n",
        "  train_loader = DataLoader(dataset, \n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=False, \n",
        "                            num_workers=0)\n",
        "  training_rbm(model, train_loader,learning_rate,n_epochs=epochs)      ## Train the layer according to its inputs ( see argument of the function \"s1,s2\")\n",
        "\n",
        "  h_1,_,_ = model(torch.from_numpy(s1).float())     ## Inference of spectra magnitude of source 1\n",
        "  h_2,_,_ = model(torch.from_numpy(s2).float())     ## Inference of spectra magnitude of source 1\n",
        "\n",
        "  return model,h_1.cpu().detach().numpy(),h_2.cpu().detach().numpy()\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YurTnAqOBA7",
        "outputId": "7c75a304-30dc-430f-f4b6-107c19cc8019"
      },
      "source": [
        "# load training data in batches\n",
        "from sklearn.preprocessing import MinMaxScaler \n",
        "\n",
        "s1_mag = np.abs(spectrum_s1).astype(\"float32\")\n",
        "s2_mag = np.abs(spectrum_s2).astype(\"float32\")\n",
        "\n",
        "s1_mag_scaled = MinMaxScaler().fit_transform(s1_mag).reshape(-1,257)\n",
        "s2_mag_scaled = MinMaxScaler().fit_transform(s2_mag).reshape(-1,257)\n",
        "Y_mag_scaled = MinMaxScaler().fit_transform(Y_mag).reshape(-1,257)\n",
        "\n",
        "\n",
        "dataset=AudioDataset(Y_mag_scaled, s1_mag_scaled, s2_mag_scaled)\n",
        "\n",
        "print('Number of samples in the audio is {}'.format(len(dataset)))\n",
        "\n",
        "### First dataloader\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(dataset, \n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=False, \n",
        "                          num_workers=0)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples in the audio is 10001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SjBf_dbOKAe"
      },
      "source": [
        "n_epochs = 150\n",
        "bs = 128\n",
        "lr = 0.001"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkpv5YLWOKWz"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldL4r8uGOLLR",
        "outputId": "9848c17a-ff98-4e66-a398-5cb30e7071f0"
      },
      "source": [
        "model_1, h1_1, h_2_1 = train_layer(257,\n",
        "                                   100,\n",
        "                                   s1_mag_scaled,\n",
        "                                   s2_mag_scaled,\n",
        "                                   learning_rate = lr,\n",
        "                                   epochs=n_epochs,\n",
        "                                   batch_size=bs)\n",
        "\n",
        "h1,x_reconstructed_1,h_reconstructed_1 = model_1(torch.from_numpy(s1_mag_scaled).float())\n",
        "torch.save(model_1.state_dict(), \"w_layer_1\")\n",
        "del model_1\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Begin Training.....\n",
            "epochs 0 MSE Loss 0.24621683359146118\n",
            "epochs 10 MSE Loss 0.09550449252128601\n",
            "Epoch    13: reducing learning rate of group 0 to 9.9990e-04.\n",
            "epochs 20 MSE Loss 0.042575813829898834\n",
            "Epoch    25: reducing learning rate of group 0 to 9.9980e-04.\n",
            "epochs 30 MSE Loss 0.020238444209098816\n",
            "Epoch    37: reducing learning rate of group 0 to 9.9970e-04.\n",
            "epochs 40 MSE Loss 0.009981383569538593\n",
            "Epoch    49: reducing learning rate of group 0 to 9.9960e-04.\n",
            "epochs 50 MSE Loss 0.005052944645285606\n",
            "Epoch    61: reducing learning rate of group 0 to 9.9950e-04.\n",
            "epochs 60 MSE Loss 0.002662060782313347\n",
            "epochs 70 MSE Loss 0.001482380204834044\n",
            "Epoch    73: reducing learning rate of group 0 to 9.9940e-04.\n",
            "epochs 80 MSE Loss 0.0009057027054950595\n",
            "Epoch    85: reducing learning rate of group 0 to 9.9930e-04.\n",
            "epochs 90 MSE Loss 0.0006265032570809126\n",
            "Epoch    97: reducing learning rate of group 0 to 9.9920e-04.\n",
            "epochs 100 MSE Loss 0.0004963790997862816\n",
            "Epoch   109: reducing learning rate of group 0 to 9.9910e-04.\n",
            "epochs 110 MSE Loss 0.00043752280180342495\n",
            "Epoch   121: reducing learning rate of group 0 to 9.9900e-04.\n",
            "epochs 120 MSE Loss 0.00041424724622629583\n",
            "epochs 130 MSE Loss 0.000407455867389217\n",
            "Epoch   133: reducing learning rate of group 0 to 9.9890e-04.\n",
            "epochs 140 MSE Loss 0.0004073061281815171\n",
            "Epoch   145: reducing learning rate of group 0 to 9.9880e-04.\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGj5faxAOWTo",
        "outputId": "12e924e4-447e-4f03-a756-9b1f970e1aac"
      },
      "source": [
        "model_2, h1_2,h_2_2 = train_layer(100,\n",
        "                                  50,\n",
        "                                  h1_1,\n",
        "                                  h_2_1,\n",
        "                                  epochs=n_epochs,\n",
        "                                  learning_rate = 0.001,\n",
        "                                  batch_size=bs)\n",
        "torch.save(model_2.state_dict(), \"w_layer_2\")\n",
        "del model_2"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Begin Training.....\n",
            "epochs 0 MSE Loss 0.5815340280532837\n",
            "epochs 10 MSE Loss 0.30040234327316284\n",
            "Epoch    13: reducing learning rate of group 0 to 9.9990e-04.\n",
            "epochs 20 MSE Loss 0.1971011906862259\n",
            "Epoch    25: reducing learning rate of group 0 to 9.9980e-04.\n",
            "epochs 30 MSE Loss 0.1537417769432068\n",
            "Epoch    37: reducing learning rate of group 0 to 9.9970e-04.\n",
            "epochs 40 MSE Loss 0.13426533341407776\n",
            "Epoch    49: reducing learning rate of group 0 to 9.9960e-04.\n",
            "epochs 50 MSE Loss 0.127436101436615\n",
            "Epoch    61: reducing learning rate of group 0 to 9.9950e-04.\n",
            "epochs 60 MSE Loss 0.12520436942577362\n",
            "epochs 70 MSE Loss 0.123199462890625\n",
            "Epoch    73: reducing learning rate of group 0 to 9.9940e-04.\n",
            "epochs 80 MSE Loss 0.12245701998472214\n",
            "Epoch    85: reducing learning rate of group 0 to 9.9930e-04.\n",
            "epochs 90 MSE Loss 0.12258145213127136\n",
            "Epoch    97: reducing learning rate of group 0 to 9.9920e-04.\n",
            "epochs 100 MSE Loss 0.12206724286079407\n",
            "Epoch   109: reducing learning rate of group 0 to 9.9910e-04.\n",
            "epochs 110 MSE Loss 0.12240894138813019\n",
            "Epoch   121: reducing learning rate of group 0 to 9.9900e-04.\n",
            "epochs 120 MSE Loss 0.12257175147533417\n",
            "epochs 130 MSE Loss 0.12126697599887848\n",
            "Epoch   133: reducing learning rate of group 0 to 9.9890e-04.\n",
            "epochs 140 MSE Loss 0.12177823483943939\n",
            "Epoch   145: reducing learning rate of group 0 to 9.9880e-04.\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pqoadauOaIY",
        "outputId": "a43a7e76-d54e-4bbe-9be2-963313f642b3"
      },
      "source": [
        "model_3, h1_3,h_2_3 = train_layer(50,\n",
        "                                  10,\n",
        "                                  h1_2,\n",
        "                                  h_2_2,\n",
        "                                  epochs=n_epochs,\n",
        "                                  learning_rate = 0.001,\n",
        "                                  batch_size=bs)\n",
        "torch.save(model_3.state_dict(), \"w_layer_3\")\n",
        "del model_3"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Begin Training.....\n",
            "epochs 0 MSE Loss 0.823908805847168\n",
            "epochs 10 MSE Loss 0.6994829177856445\n",
            "Epoch    13: reducing learning rate of group 0 to 9.9990e-04.\n",
            "epochs 20 MSE Loss 0.5778981447219849\n",
            "Epoch    25: reducing learning rate of group 0 to 9.9980e-04.\n",
            "epochs 30 MSE Loss 0.4627338945865631\n",
            "Epoch    37: reducing learning rate of group 0 to 9.9970e-04.\n",
            "epochs 40 MSE Loss 0.36975908279418945\n",
            "Epoch    49: reducing learning rate of group 0 to 9.9960e-04.\n",
            "epochs 50 MSE Loss 0.29953598976135254\n",
            "Epoch    61: reducing learning rate of group 0 to 9.9950e-04.\n",
            "epochs 60 MSE Loss 0.25207841396331787\n",
            "epochs 70 MSE Loss 0.22157610952854156\n",
            "Epoch    73: reducing learning rate of group 0 to 9.9940e-04.\n",
            "epochs 80 MSE Loss 0.20041602849960327\n",
            "Epoch    85: reducing learning rate of group 0 to 9.9930e-04.\n",
            "epochs 90 MSE Loss 0.19017481803894043\n",
            "Epoch    97: reducing learning rate of group 0 to 9.9920e-04.\n",
            "epochs 100 MSE Loss 0.18281292915344238\n",
            "Epoch   109: reducing learning rate of group 0 to 9.9910e-04.\n",
            "epochs 110 MSE Loss 0.17769327759742737\n",
            "Epoch   121: reducing learning rate of group 0 to 9.9900e-04.\n",
            "epochs 120 MSE Loss 0.1745128035545349\n",
            "epochs 130 MSE Loss 0.1745114028453827\n",
            "Epoch   133: reducing learning rate of group 0 to 9.9890e-04.\n",
            "epochs 140 MSE Loss 0.17366349697113037\n",
            "Epoch   145: reducing learning rate of group 0 to 9.9880e-04.\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owcYRrkAOpdK"
      },
      "source": [
        "# DNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNTwEQrFOp8s"
      },
      "source": [
        "class DNN(nn.Module):\n",
        "\n",
        "    def __init__(self,d):\n",
        "        super(DNN, self).__init__()\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        self.fc1 = nn.Linear(d, 100,bias=False)  # d is dimension of the input.\n",
        "        self.fc2 = nn.Linear(100, 50,bias=False)\n",
        "        self.fc3 = nn.Linear(50, 10,bias=False)\n",
        "        self.fc4 = nn.Linear(10,2,bias=False)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.sigmoid(self.fc1(x))\n",
        "        x = self.sigmoid(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))\n",
        "        x = self.sigmoid(self.fc4(x))\n",
        "\n",
        "\n",
        "        return x"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CBFdtZPOtGX",
        "outputId": "ead91826-d718-41b8-96ec-24283cf0ad0a"
      },
      "source": [
        "dnn_model = DNN(d=257)\n",
        "model_1 = RBM(257,100)\n",
        "model_1.load_state_dict(torch.load(\"w_layer_1\"))\n",
        "model_2 = RBM(100,50)\n",
        "model_2.load_state_dict(torch.load(\"w_layer_2\"))\n",
        "model_3 = RBM(50,10)\n",
        "model_3.load_state_dict(torch.load(\"w_layer_3\"))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT1bSkRXO0Zm"
      },
      "source": [
        "with torch.no_grad():\n",
        "    dnn_model.fc1.weight = model_1.feed.weight\n",
        "    dnn_model.fc2.weight = model_2.feed.weight\n",
        "    dnn_model.fc3.weight = model_3.feed.weight"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy0AwAhRDp1t"
      },
      "source": [
        "from  helpers_dnn import *\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZJt9micZvX9",
        "outputId": "3b5141ab-4e69-4a5a-cbb3-145b3eb8d3b7"
      },
      "source": [
        "print(torch.cuda.is_available())\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddB8LnIRhV_k"
      },
      "source": [
        "def validation_dnn(s1,s2,u,v,smr):\n",
        "\n",
        "  mixed_signal , speech_signal, music_signal = get_mixed_signal(test_speech,test_music,smr)\n",
        "  f,t,mixed_spectrum = signal.stft(mixed_signal,window=WINDOW,nperseg=WINDOW_SIZE,noverlap=OVERLAP,nfft=NFFT) # spectrum  of mixed signal\n",
        "  #mixed_spectrum_ = MinMaxScaler().fit_transform(np.abs(mixed_spectrum))\n",
        "  new_s1 = (u*s1).cpu().detach().numpy().reshape(257,-1)\n",
        "  new_s2 = (v*s2).cpu().detach().numpy().reshape(257,-1)\n",
        "\n",
        "  s1_new = (mixed_spectrum*new_s1) /(new_s1 + new_s2 +0.000001 )\n",
        "  s2_new = (mixed_spectrum*new_s2) /(new_s1 + new_s2 +0.000001)\n",
        "\n",
        "\n",
        "  _, speech_estimate =  signal.istft( s1_new,samplerate,window=WINDOW,nperseg=WINDOW_SIZE,noverlap=OVERLAP,nfft=NFFT)\n",
        "  _, music_estimate =  signal.istft( s2_new,samplerate,window=WINDOW,nperseg=WINDOW_SIZE,noverlap=OVERLAP,nfft=NFFT)\n",
        "\n",
        "  print(\"Separation finish ! ....\")\n",
        "  #print(speech_estimate.shape, speech.shape)\n",
        "  sdr_speech = SDR(speech_estimate[:speech_signal.shape[0]],speech_signal)\n",
        "  sdr_music = SDR(music_estimate[:music_signal.shape[0]],music_signal)\n",
        "\n",
        "  print(\"SDR Speech = {}\\n SDR Music =Â {}\".format(sdr_speech,sdr_music))"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW_lXLBSZOgr"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "spectrum_s1_ = scaler.fit_transform(np.abs(spectrum_s1).reshape(-1,257))\n",
        "spectrum_s2_ = MinMaxScaler().fit_transform(np.abs(spectrum_s2.reshape(-1,257)))\n",
        "Y_mag_ = MinMaxScaler().fit_transform(Y_mag.reshape(-1,257))\n",
        "\n",
        "s1 = torch.tensor(spectrum_s1_, dtype = torch.float32)\n",
        "s2 = torch.tensor(spectrum_s2_,dtype=torch.float32)\n",
        "Yabs = torch.tensor(Y_mag_,dtype=torch.float32)\n",
        "u,v = gain_params_vec(s1, s2, Yabs)\n",
        "model = DNN(257)\n",
        "\n",
        "from torch.optim import LBFGS, Adam,SGD\n",
        "s1 = s1.to(device)\n",
        "s2 = s2.to(device)\n",
        "u = u.to(device)\n",
        "v = v.to(device)\n",
        "Yabs = Yabs.to(device)\n",
        "\n",
        "s1.requires_grad = True\n",
        "s2.requires_grad = True\n",
        "u.requires_grad =  True\n",
        "v.requires_grad =  True\n",
        "Yabs.requires_grad = False\n",
        "model = model.to(device)\n",
        "\n",
        "params = list(model.parameters()) + [u] + [v] + [s1] + [s2]\n",
        "optimizer= SGD(params, lr=0.001)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpA1UwvJ022_",
        "outputId": "1b56ef56-b2a6-4caa-88af-c3899f98eaac"
      },
      "source": [
        "L2_LOSS=torch.norm(u*s1 + v*s2 - Yabs,'fro',None)\n",
        "print('Initial Loss = {}'.format(L2_LOSS))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial Loss = 36.16548538208008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mZFmfhoWzG7"
      },
      "source": [
        "def Criteria(s1,s2,Yabs,u,v,i,model,beta,l):\n",
        "    \n",
        "        # Feed forward and get energy 1 and 2\n",
        "    e1 = energy_1(s1[i,:].float(),model)\n",
        "    e2 = energy_2(s2[i,:].float(),model)\n",
        "\n",
        "    # Get least square error :\n",
        "    e_rr=E_err_vec(s1[i,:], s2[i,:], Yabs[i,:],u[i],v[i])\n",
        "\n",
        "    # Non negative constraint\n",
        "    R = nonneg_constraint_sum(s1[i,:], s2[i,:], u[i], v[i])\n",
        "    \n",
        "    return e1 + e2 + l*e_rr + beta*R"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdFxqwJmZSce"
      },
      "source": [
        "optimizer= SGD(params, lr=0.005)\n",
        "N_epochs = 1000\n",
        "\n",
        "model.train()\n",
        "qq = []\n",
        "Yabs = Yabs.to(device)\n",
        "for e in range(N_epochs):\n",
        "  loss_e=0.0\n",
        "\n",
        "\n",
        "  model.fc1.weight.requires_grad = True\n",
        "  model.fc2.weight.requires_grad = True\n",
        "  model.fc3.weight.requires_grad = True\n",
        "\n",
        "  for i in range(s1.shape[1]):\n",
        "\n",
        "            \n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          loss = Criteria(s1,s2,Yabs,u,v,i,model,3,5).to(device)\n",
        "          loss.backward()\n",
        "          # fill zeros into the first row of grad\n",
        "          u.grad.data[:i].fill_(0)\n",
        "          u.grad.data[i+1:].fill_(0)\n",
        "          \n",
        "          v.grad.data[:i].fill_(0)\n",
        "          v.grad.data[i+1:].fill_(0)\n",
        "          s1.grad.data[:,:i].fill_(0)\n",
        "          s1.grad.data[:,i+1:].fill_(0)\n",
        "          s2.grad.data[:,:i].fill_(0)\n",
        "          s2.grad.data[:,i+1:].fill_(0)\n",
        "\n",
        "\n",
        "\n",
        "          optimizer.step()\n",
        "          loss_e+=loss\n",
        "  \n",
        "  if e%5 == 0:    \n",
        "    print('Epoch {} , Loss = {:.2f}'.format(e,loss_e/s1.shape[1]))\n",
        "    validation_dnn(s1,s2,u,v,-5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anZTEVLowMk-"
      },
      "source": [
        "SDR Speech = -0.8132213434622194\n",
        "SDR Music  = 4.1867786565377765\n",
        "\n",
        "SDR Speech = -0.69\n",
        " SDR Music = 4.31"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "kIC2yBQQLHcK",
        "outputId": "dae1e2e1-7a68-4408-a2a6-a0656c138332"
      },
      "source": [
        "torch.save(\"model_5\",model)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c932ef339594>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5hQxbxqLLY7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}