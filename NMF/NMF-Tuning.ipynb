{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io.wavfile import read, write\n",
    "from scipy import signal\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy.linalg import inv\n",
    "from helpers import Reconstruct, Viz_Y,SMR,get_mixed_signal,SDR\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import torch\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Read speech and music data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the speech 100000\n",
      "Length : 2.27s\n",
      "Sample rate : 44100\n",
      "Shape of the music 100000\n",
      "Length : 2.27s\n",
      "Sample rate : 44100\n"
     ]
    }
   ],
   "source": [
    "samplerate_s, data_speech = read(\"../data/male_vocal.wav\")\n",
    "speech=data_speech[:100000,0]\n",
    "length=speech.shape[0]/samplerate_s\n",
    "print('Shape of the speech {}'.format(speech.shape[0]))\n",
    "print('Length : {:.2f}s'.format(length))\n",
    "print('Sample rate : {}'.format(samplerate_s))\n",
    "\n",
    "samplerate_m, data_music = read(\"../data/piano.wav\")\n",
    "music=data_music[:speech.shape[0],0]\n",
    "length=music.shape[0]/samplerate_m\n",
    "print('Shape of the music {}'.format(music.shape[0]))\n",
    "print('Length : {:.2f}s'.format(length))\n",
    "print('Sample rate : {}'.format(samplerate_m))\n",
    "\n",
    "samplerate_t, test = read(\"../data/mixed_signal.wav\")\n",
    "test=test[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed,_,_=get_mixed_signal(speech, music, 10)\n",
    "write(\"../../mixed.wav\", samplerate_s, mixed.astype(np.int16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply STFT :\n",
    "\n",
    "### We can change :\n",
    "\n",
    "* Window : Type of window\n",
    "* nperseg : length of window\n",
    "* noverlap : overlap between windows.\n",
    "* nfft : fft length > window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test,speech,music=get_mixed_signal(speech,music,10)\n",
    "\n",
    "WINDOW = 'hamming'\n",
    "WINDOW_SIZE=480\n",
    "OVERLAP = 0.6 * WINDOW_SIZE\n",
    "NFFT=512\n",
    "\n",
    "f,t,Y= signal.stft(speech,samplerate_s,window=WINDOW,nperseg=WINDOW_SIZE,noverlap=OVERLAP,nfft=NFFT)\n",
    "Yabs_s=np.abs(Y)\n",
    "\n",
    "f,t,Y= signal.stft(music,samplerate_m,window=WINDOW,nperseg=WINDOW_SIZE,noverlap=OVERLAP,nfft=NFFT)\n",
    "Yabs_m=np.abs(Y)\n",
    "\n",
    "f,t,Y= signal.stft(test,samplerate_t,window=WINDOW,nperseg=WINDOW_SIZE,noverlap=OVERLAP,nfft=NFFT)\n",
    "Yabs_t=np.abs(Y)\n",
    "print('Shape of spectrogram speech : {}'.format(Yabs_s.shape))\n",
    "print('Shape of spectrogram music: {}'.format(Yabs_m.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If we apply the elbow method the optimal number of componenets will be 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yabs_s[Yabs_s==0]=0.0001\n",
    "Yabs_t[Yabs_t==0]=0.0001\n",
    "Yabs_m[Yabs_m==0]=0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluation(speech, music,Ns,Nm,SMR_db,samplerate):\n",
    "    \n",
    "    \n",
    "    test,speech,music=get_mixed_signal(speech,music,SMR_db)\n",
    "\n",
    "    WINDOW = 'hamming'\n",
    "    WINDOW_SIZE=480\n",
    "    OVERLAP = 0.6 * WINDOW_SIZE\n",
    "    NFFT=512\n",
    "\n",
    "    f,t,Y= signal.stft(speech,samplerate,window=WINDOW,nperseg=WINDOW_SIZE,noverlap=OVERLAP,nfft=NFFT)\n",
    "    Yabs_s=np.abs(Y)\n",
    "\n",
    "    f,t,Y= signal.stft(music,samplerate,window=WINDOW,nperseg=WINDOW_SIZE,noverlap=OVERLAP,nfft=NFFT)\n",
    "    Yabs_m=np.abs(Y)\n",
    "\n",
    "    f,t,Y= signal.stft(test,samplerate,window=WINDOW,nperseg=WINDOW_SIZE,noverlap=OVERLAP,nfft=NFFT)\n",
    "    Yabs_t=np.abs(Y)\n",
    "    \n",
    "    Yabs_s[Yabs_s==0]=0.0001\n",
    "    Yabs_t[Yabs_t==0]=0.0001\n",
    "    Yabs_m[Yabs_m==0]=0.0001\n",
    "\n",
    "    model = NMF(n_components=Ns, init='random',alpha=0.0,beta_loss='itakura-saito',solver=\"mu\",max_iter=200, random_state=0)\n",
    "    G_s = model.fit_transform(np.transpose(Yabs_s))\n",
    "    B_s = model.components_\n",
    "    model = NMF(n_components=Nm, init='random',alpha=0.0,beta_loss='itakura-saito',solver=\"mu\",max_iter=200, random_state=0)\n",
    "    G_m = model.fit_transform(np.transpose(Yabs_m))\n",
    "    B_m = model.components_\n",
    "\n",
    "    B=np.vstack([B_s,B_m])\n",
    "    model_test = NMF(n_components=Ns+Nm, init='random',alpha=0.0,beta_loss='itakura-saito',solver=\"mu\",max_iter=200, random_state=0)\n",
    "    model_test.fit(np.transpose(Yabs_t))\n",
    "    \n",
    "    model_test.components_=B\n",
    "    G_test=model_test.transform(np.transpose(Yabs_t))\n",
    "    \n",
    "    \n",
    "    Sources,Masks=Reconstruct(B=np.transpose(B),G=np.transpose(G_test),Ns=Ns,Nm=Nm,Yabs=Y,p=2)\n",
    "    \n",
    "    speech_est = Sources[0]\n",
    "    music_est = Sources[1]\n",
    "    \n",
    "    _, speech_est =  signal.istft(speech_est,\n",
    "                          samplerate,\n",
    "                          window = WINDOW,\n",
    "                          nperseg=WINDOW_SIZE,\n",
    "                          noverlap=OVERLAP,\n",
    "                          nfft = NFFT)\n",
    "    \n",
    "    _, music_est =  signal.istft(music_est,\n",
    "                          samplerate,\n",
    "                          window = WINDOW,\n",
    "                          nperseg=WINDOW_SIZE,\n",
    "                          noverlap=OVERLAP,\n",
    "                          nfft = NFFT)\n",
    "    \n",
    "    speech_est = speech_est[:speech.shape[0]]\n",
    "    music_est = music_est[:music.shape[0]]\n",
    "    \n",
    "    sdr_speech = SDR(s_est=speech_est,s=speech)\n",
    "    sdr_music = SDR(s_est=music_est, s=music)\n",
    "    \n",
    "    return sdr_speech, sdr_music\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation SMR = -5 Ns = Nm = 2\n",
      "SMR = -5.00\n",
      "Speech SDR = 0.19582603420746408 ... Music SDR = 5.195826034207477\n",
      "Evaluation SMR = -5 Ns = Nm = 16\n",
      "SMR = -5.00\n",
      "Speech SDR = 0.5226139877333418 ... Music SDR = 5.522613987733356\n",
      "Evaluation SMR = -5 Ns = Nm = 32\n",
      "SMR = -5.00\n",
      "Speech SDR = 0.3215587291009167 ... Music SDR = 5.321558729100929\n",
      "Evaluation SMR = -5 Ns = Nm = 64\n",
      "SMR = -5.00\n",
      "Speech SDR = 0.4577974691203778 ... Music SDR = 5.45779746912039\n",
      "Evaluation SMR = -5 Ns = Nm = 128\n",
      "SMR = -5.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|████████████████████████████                                                        | 1/3 [00:15<00:31, 15.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech SDR = 0.782115245989678 ... Music SDR = 5.782115245989691\n",
      "Evaluation SMR = 0 Ns = Nm = 2\n",
      "Speech SDR = 1.845279041389463 ... Music SDR = -1.7639280969679798\n",
      "Evaluation SMR = 0 Ns = Nm = 16\n",
      "Speech SDR = 5.181001282810592 ... Music SDR = 1.5717941115601848\n",
      "Evaluation SMR = 0 Ns = Nm = 32\n",
      "Speech SDR = 4.908447082149917 ... Music SDR = 1.2992399464111961\n",
      "Evaluation SMR = 0 Ns = Nm = 64\n",
      "Speech SDR = 3.0694553728398946 ... Music SDR = -0.5397517543050638\n",
      "Evaluation SMR = 0 Ns = Nm = 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|████████████████████████████████████████████████████████                            | 2/3 [00:31<00:15, 15.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech SDR = 2.8672957298735593 ... Music SDR = -0.7419113672207176\n",
      "Evaluation SMR = 5 Ns = Nm = 2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'numpy.ndarray' and 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-7c7d02d4152a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Evaluation SMR = {} Ns = Nm = {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSMR\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0msdr_speech\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msdr_music\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEvaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspeech\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspeech\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmusic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmusic\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSMR_db\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSMR\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msamplerate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msamplerate_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[0mSDR_SPEECH_Ncomp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msdr_speech\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mSDR_MUSIC_Ncomp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msdr_music\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-f3e90acef4d1>\u001b[0m in \u001b[0;36mEvaluation\u001b[1;34m(speech, music, Ns, Nm, SMR_db, samplerate)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mspeech\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmusic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_mixed_signal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspeech\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmusic\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSMR_db\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mWINDOW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'hamming'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\PFE\\Git\\Deep-Learning-Based-Blind-Source-Separation\\NMF\\helpers.py\u001b[0m in \u001b[0;36mget_mixed_signal\u001b[1;34m(speech, music, SMR_db)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mSMR_db\u001b[0m \u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mmixed\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mspeech\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmusic\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[0mmusic_scaled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmusic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mSMR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspeech\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmusic_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'numpy.ndarray' and 'Tensor'"
     ]
    }
   ],
   "source": [
    "SDR_MUSIC=[]\n",
    "SDR_SPEECH=[]\n",
    "\n",
    "for SMR in tqdm([-5,0,5]):\n",
    "    \n",
    "    SDR_MUSIC_Ncomp=[]\n",
    "    SDR_SPEECH_Ncomp=[]\n",
    "    \n",
    "    for Ns in [2,16,32,64,128]:\n",
    "            \n",
    "            print('Evaluation SMR = {} Ns = Nm = {}'.format(SMR,Ns))\n",
    "            sdr_speech,sdr_music=Evaluation(speech=speech, music=music,Ns=Ns,Nm=Ns,SMR_db=SMR,samplerate=samplerate_s)\n",
    "            SDR_SPEECH_Ncomp.append(sdr_speech)\n",
    "            SDR_MUSIC_Ncomp.append(sdr_music)\n",
    "            print('Speech SDR = {} ... Music SDR = {}'.format(sdr_speech,sdr_music))\n",
    "            \n",
    "    SDR_MUSIC.append(SDR_MUSIC_Ncomp)\n",
    "    SDR_SPEECH.append(SDR_SPEECH_Ncomp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDR_MUSIC_ARRAY=np.array(SDR_MUSIC)\n",
    "SDR_SPEECH_ARRAY=np.array(SDR_SPEECH)\n",
    "np.save('./SDR/Music_sdr',SDR_MUSIC_ARRAY)\n",
    "np.save('./SDR/Speech_sdr',SDR_SPEECH_ARRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation of the sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    \n",
    "    _, xrec =  signal.istft(Sources[i],\n",
    "                          samplerate_t,\n",
    "                          window = WINDOW,\n",
    "                          nperseg=WINDOW_SIZE,\n",
    "                          noverlap=OVERLAP,\n",
    "                          nfft = NFFT)\n",
    "    write(\"../../example\"+str(i)+\".wav\", samplerate_t, xrec.astype(np.int16))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
